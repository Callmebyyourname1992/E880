
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{caption} %For captioning objects
\usepackage{subcaption} %sub-captioning pictures
\usepackage{graphicx} %to include graphics
\usepackage{hyperref} %for clickable references
\usepackage{listings} %to write code listings
\lstset{language=R, breaklines=true}  
\usepackage[mathcal]{euscript} %for curly S
\usepackage{mathtools}
\usepackage{float} %so figures can be placed "here"

%Defining commands for math symbols
\usepackage{amsmath} %to enable split equations
\usepackage{statmath} %for plim. Be careful, it has already \E and \V.
\usepackage{amssymb} %to enable mathbb
\newcommand{\R}{\mathtt{R}} %Software R
\renewcommand{\E}{\mathbb{E}} %expectation
\usepackage{bbm}
\newcommand{\1}{\mathbbm{1}}
\renewcommand{\V}{\mathbb{V}} %variance
\newcommand{\N}{\mathcal{N}} %normal distribution
\newcommand{\U}{\mathcal{U}} %normal distribution
\renewcommand{\P}{\mathbb{P}} %proba, renewcom since \P already exists
%Regression variables in vector form
\newcommand{\y}{\boldsymbol{y}} 
\newcommand{\x}{\boldsymbol{x}} 
\newcommand{\z}{\boldsymbol{z}} 
\newcommand{\yhat}{\boldsymbol{\hat{y}}} 
\renewcommand{\u}{\boldsymbol{u}} 
\newcommand{\uhat}{\boldsymbol{\hat{u}}}  
\newcommand{\Px}{\boldsymbol{P}}  
\newcommand{\Mx}{\boldsymbol{M}}  
\newcommand{\A}{\boldsymbol{A}}  
\newcommand{\X}{\boldsymbol{X}}  
\newcommand{\Z}{\boldsymbol{Z}}  
\newcommand{\e}{\boldsymbol{e}}  
\renewcommand{\r}{\tilde{r}}  
%\renewcommand{\r}{\boldsymbol{r}} 
\renewcommand{\i}{\boldsymbol{\imath}} 
\newcommand{\alphab}{\boldsymbol{\alpha}}  
\newcommand{\betab}{\boldsymbol{\beta}}  
%opening

\newcounter{daggerfootnote}
\newcommand*{\daggerfootnote}[1]{%
	\setcounter{daggerfootnote}{\value{footnote}}%
	\renewcommand*{\thefootnote}{\fnsymbol{footnote}}%
	\footnote[2]{#1}%
	\setcounter{footnote}{\value{daggerfootnote}}%
	\renewcommand*{\thefootnote}{\arabic{footnote}}%
}


\title{Problem Set 1 - ECON 880\\
	\small Spring 2022 - University of Kansas}
\author{Minh Cao, Gunawan}


\begin{document}

\maketitle	
\section*{Problem 1}
\subsection*{(1a)}
We can evaluate the polynomial as follow:\\
\begin{eqnarray*}
f(x,y) 	&=& 83521y^8+578x^{2}y^{4} -2x^{4}+2x^{6}-x^{8}\\
 		&=& y^{4}(83521y^{4}+578x^{2})+x^{4}(-2+2x^{2}-x^{4})
\end{eqnarray*}
\subsection*{(1b)}
We estimate the order of magnitude (number of digits) of 
\begin{equation}\label{eq:1b}
83521y^8,
\end{equation}
where $y=2298912$. We can rewrite the expression as follows:
	\begin{eqnarray*}
		83521y^8 &=& 83521\cdot 2298912^8\\
				 &=& 8.3521\cdot10^4 \cdot (2.298912\cdot10^6)^8
	\end{eqnarray*}
Then, the digits can be estimated by using the following formula 
	\begin{eqnarray*}
		1+\lfloor \log_{10} (83521\cdot 2298912^8)\rfloor &=& 1+\lfloor \log_{10} (83521\cdot 2298912^8)\rfloor\\
		&=&  1+\lfloor \log_{10} (8.3521\cdot10^4 \cdot (2.298912\cdot10^6)^8)\rfloor\\
			&=&  1+\lfloor \log_{10} 8.3521+\log_{10}10^4 + \log_{10}2.298912^8 +\log_{10} (10^6)^8\rfloor\\
			&=& 56
	\end{eqnarray*} 
Now, we compute the last digits of that term, first by plugging in only the last digit of $y$ into expression \eqref{eq:1b}: our calculation gives 21312256 as a result. However, printing out the complete term using the command $\text{fprintf}('\%.0f\backslash n',83251*y^8)$, we obtain 
\[64,948,367,112,155,225,808,364,994,454,077,172,753,899,719,772,317,155,328.\] 
We see that both results have different last digit $(6\neq8)$.




\subsection*{(1c)}
	test 2
\section*{Problem 2}
In this exercise, we write an algorithm to determine the relative speeds of addition, multiplication, division, exponentiation, and the logarithmic function of our computer. The computer uses Intel(R) Core(TM) i5-1035G4 CPU @ 1.10GHz, 1.50 GHz with 64-bit operating system and 8.00 GB installed RAM. We proceed by generating two matrices $A$ and $B$ of size $10^4\times10^4$ using the rand() function in Matlab. Then, we use them in element-wise operations of addition ($A+B$), multiplication ($A.*B$), division ($A./B$), exponentiation ($A.^B$), as well as the logarithmic function ($\log(A)$). The statements \texttt{tic} and \texttt{toc} are used around them to measure the computation time. This procedure is iterated 100 times, and the average computation time for each operation is computed. The results are as follows:
	\begin{itemize}
	\item Average computation time for variable initiation is 1.54030 seconds
	\item Average computation time for addition is 0.12073 seconds
	\item Average computation time for multiplication is 0.12083 seconds
	\item Average computation time for division is 0.11823 seconds
	\item Average computation time for exponentiation is 3.70586 seconds
	\item Average computation time for log function is 0.91528 seconds
	\end{itemize}
\section*{Problem 3}
	In order to find our machine $\varepsilon$, we follow Ken Judd's definition\daggerfootnote{Kenneth L. Judd, 1998. "Numerical Methods in Economics," MIT Press Books, The MIT Press, p.30} by writing a while loop to subtract (resp. add) progressively smaller numbers $\epsilon$ from (resp. to) 1 until the condition $1+\epsilon > 1 > 1-\epsilon$ is no longer satisfied. Repeat the exercise using 0.001 and 1000 instead of 1. We verify our results by comparing them with the epsilons delivered by the built-in Matlab function \texttt{eps(x)}, and the exponents with \texttt{log2(eps(x))}. The results are shown on Table \ref{tab:1}.
	
	\begin{table}[h]
		\centering
		\begin{tabular}{ | c | c | c |}
			\hline
			\hline
			$x$ & $\varepsilon(x)$ - decimal & $\varepsilon(x)$ - exponential \\	
			\hline
			0.001 & 2.16840434497101e-19 & $2^{-62}$ \\ \hline
			1 & 2.22044604925031e-16 & $2^{-52}$ \\ \hline
			1000 & 1.13686837721616e-13 & $2^{-43}$ \\
			\hline
			\hline
		\end{tabular} 
		\caption{Machine $\varepsilon$ for diverse values of $x$}
		\label{tab:1}
	\end{table}
Comment: As $x$ increases, the machine $\varepsilon(x)$ also does increase. This is to be expected, since $\varepsilon(x)\approx x\varepsilon(1)$. Thus, $\varepsilon(1000)>\varepsilon(1)>\varepsilon(0.001)$.
	
\section*{Problem 4} 
We wrote a loop to evaluate the convergence of the following sequences: 
\begin{enumerate}
	\item[(4a)] $x_k=\sum_{k=1}^{n}\frac{1}{2^n}$, where $\lim_{k\rightarrow\infty}x_k=1$
	\item[(4b)] $y_k=\sum_{k=1}^{n}\frac{1}{n}$, where $\lim_{k\rightarrow\infty}y_k=\infty$
\end{enumerate}
We use absolute and relative convergence criteria, and tolerance distance $\delta\in\{10^{-2},10^{-4},10^{-6}\}$ as stopping rule. We limit the maximum number of iterations to 100,000. The number of iterations before convergence, as well the final guess are reported on Table \ref{tab:2} and \ref{tab:3} for $x_k$ and $y_k$, respectively.

	\begin{table}[h]
	\centering
	\begin{tabular}{|c | c | c | c |}
		\hline
		\hline
		$\delta	$							&$10^{-2}$ 			&$10^{-4}$ 			&$10^{-6}$          \\
		\hline
		no. of iteration - absolute criteria&7					&14					&20               \\
		final guess - absolute criteria		&0.992187500000000	&0.999938964843750	&0.999999046325684\\
		no. of iteration - relative criteria&7					&14					&20               \\
		final guess - relative criteria		&0.992187500000000	&0.999938964843750	&0.999999046325684\\
		\hline
		\hline
	\end{tabular} 
	\caption{Convergence for $x_k$}
	\label{tab:2}
\end{table}

	\begin{table}[h]
	\centering
	\begin{tabular}{|c | c | c | c |}
		\hline
		\hline
		$\delta	$							&$10^{-2}$ 			&$10^{-4}$ 			&$10^{-6}$          \\
		\hline
		no. of iteration - absolute criteria&100					&10,000				&100,000 \\
		final guess - absolute criteria		&5.18737751763962	&9.78760603604435	&12.0901461298633\\
		no. of iteration - relative criteria&100					&10,000				&100,000 \\
		final guess - relative criteria		&5.18737751763962	&9.78760603604435	&12.0901461298633\\
		\hline
		\hline
	\end{tabular} 
	\caption{Convergence for $y_k$}
	\label{tab:3}
\end{table}

Comment: Table \ref{tab:2} shows that both absolute and relative convergence criteria lead to the same number of iterations for the sequence $x_k$. As tolerance distance $\delta$ lowers, the final guess for $x_k$ approaches its true limit 1. Table \ref{tab:3} also shows that both absolute and relative criteria lead to the same number of iterations for the sequence $y_k$. Since $y_k$ is a divergent sequence, the final guess will be higher (with no upper bound), the more we iterate. By lowering tolerance distance $\delta$, the algorithm needs to iterate longer in order to satisfy the stopping rule, which in turn results in higher final guess. Since there is no upper bound for $y_k$, we can always make the tolerance distance $\delta$ even lower, and obtain even higher final guess for $y_k$.

\end{document}

