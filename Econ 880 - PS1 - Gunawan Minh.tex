
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{caption} %For captioning objects
\usepackage{subcaption} %sub-captioning pictures
\usepackage{graphicx} %to include graphics
\usepackage{hyperref} %for clickable references
\usepackage{listings} %to write code listings
\lstset{language=R, breaklines=true}  
\usepackage[mathcal]{euscript} %for curly S
\usepackage{mathtools}
\usepackage{float} %so figures can be placed "here"

%Defining commands for math symbols
\usepackage{amsmath} %to enable split equations
\usepackage{statmath} %for plim. Be careful, it has already \E and \V.
\usepackage{amssymb} %to enable mathbb
\newcommand{\R}{\mathtt{R}} %Software R
\renewcommand{\E}{\mathbb{E}} %expectation
\usepackage{bbm}
\newcommand{\1}{\mathbbm{1}}
\renewcommand{\V}{\mathbb{V}} %variance
\newcommand{\N}{\mathcal{N}} %normal distribution
\newcommand{\U}{\mathcal{U}} %normal distribution
\renewcommand{\P}{\mathbb{P}} %proba, renewcom since \P already exists
%Regression variables in vector form
\newcommand{\y}{\boldsymbol{y}} 
\newcommand{\x}{\boldsymbol{x}} 
\newcommand{\z}{\boldsymbol{z}} 
\newcommand{\yhat}{\boldsymbol{\hat{y}}} 
\renewcommand{\u}{\boldsymbol{u}} 
\newcommand{\uhat}{\boldsymbol{\hat{u}}}  
\newcommand{\Px}{\boldsymbol{P}}  
\newcommand{\Mx}{\boldsymbol{M}}  
\newcommand{\A}{\boldsymbol{A}}  
\newcommand{\X}{\boldsymbol{X}}  
\newcommand{\Z}{\boldsymbol{Z}}  
\newcommand{\e}{\boldsymbol{e}}  
\renewcommand{\r}{\tilde{r}}  
%\renewcommand{\r}{\boldsymbol{r}} 
\renewcommand{\i}{\boldsymbol{\imath}} 
\newcommand{\alphab}{\boldsymbol{\alpha}}  
\newcommand{\betab}{\boldsymbol{\beta}}  
%opening

\newcounter{daggerfootnote}
\newcommand*{\daggerfootnote}[1]{%
	\setcounter{daggerfootnote}{\value{footnote}}%
	\renewcommand*{\thefootnote}{\fnsymbol{footnote}}%
	\footnote[2]{#1}%
	\setcounter{footnote}{\value{daggerfootnote}}%
	\renewcommand*{\thefootnote}{\arabic{footnote}}%
}


\title{Problem Set 1 - ECON 880\\
	\small Spring 2022 - University of Kansas}
\author{Gunawan, Minh Cao}


\begin{document}

\maketitle	
\section*{Problem 1}
We want to compute
\begin{equation}\label{eq:1}
	83521y^8 +578x^2 y^4 -2x^4 +2x^6 -x^8
\end{equation}
for $x = 9478657$ and $y = 2298912$.
\subsection*{(1a)}
We evaluate the expression \eqref{eq:1} as it is written, and then by grouping terms using Horner's method. The Horner's factorization gives:

\begin{equation*}
	83521y^8+x(0+x(578y^4+x(0+x(-2+x(0+x(2+x(0+x(-1))))))))
\end{equation*}
Horner's algorithm for this exercise is coded in Matlab according to Kenneth L. Judd, 1998. "Numerical Methods in Economics," MIT Press Books, The MIT Press, p.35. Our results indicate that both direct calculation and Horner's method lead to the same quantity $f(x,y) = -1.088903574147003\cdot 10^{40}$. However, the calculation time for Horner's method is much less than direct method ($0.016712 < 0.099468$ seconds), as we have less exponentiation to do in the former method. Below is the summary for the operations performed using both method.

	\begin{table}[h]
	\centering
	\begin{tabular}{l  c c c }
		\hline
		\hline
						& Additions & Multiplications & Exponentiations \\
		\hline
		Direct Method 	&4			&6&6\\
		Horner's Method &4			&11&2\\
		\hline
		\hline
	\end{tabular} 
	\caption{Polynomial evaluation costs}
	\label{tab:0}
\end{table}

%The multivariate Honer's algorithm can be described as follows:\\
%\begin{itemize}
%\item Step 1: Let our original multivariable polynomial be $f(x,y) = M_{1} +M_{2}+M_{3}+M_{4}+M_{5}$, where $M_{i}$ is a monomial, for all $i=1,\ldots,5$. In this case, 
%\[M_{1} = 83521y^{8}, M_{2} = 578x^{2}y^{4}, M_{3}= -2x^{4}, M_{4}=2x^{6}, M_{5} = -x^{8}.\]
%\item Step 2: For each monomial $M_{i}$, we count, for each variable $x$ and $y$, the number of monomials $M_{i}$ that contain this variable. We then select the variable with the largest value of $M_{i}$: we choose $x$, as it appears 4 times, while $y$ only twice. Rewrite the original polynomial as $f(x,y) = A_{0}+A_{1}x$. Where $A_{0}$ is the monomial that does not contain $x^2$, and $A_{1}$ is the sum of all the terms ${M_{k}}' = \frac{M_{k}}{x} $ corresponding to all the monomials $M_{k}$ that do contain $x$. In this case, $A_{0} = 83521y^8$ and $A_{1} = 578xy^{4} - 2x^3+2x^{5}-x^{7}$, or
%\[f(x,y) = 83521y^8 +x(578xy^{4} - 2x^{3}+2x^{5}-x^{7}) \]

%\item Step 3: Repeat step 1 and 2 until we get:
%\begin{eqnarray*}
%	f(x,y) &=& 83521y^8 +xx(578y^{4} - 2x^{2}+2x^{4}-x^{6}) \\
%		   &=& ...\\
%		   &=& 83521y^8 +xx (578y^{4} +xx(-2+xx(2-xx)))
%\end{eqnarray*}

%\end{itemize}

\subsection*{(1b)}
We estimate the order of magnitude (number of digits) of 
\begin{equation}\label{eq:1b}
83521y^8,
\end{equation}
where $y=2298912$. We can rewrite the expression as follows:
	\begin{eqnarray*}
		83521y^8 &=& 83521\cdot 2298912^8\\
				 &=& 8.3521\cdot10^4 \cdot (2.298912\cdot10^6)^8
	\end{eqnarray*}
Then, the digits can be estimated by using the following formula 
	\begin{eqnarray*}
		1+\lfloor \log_{10} (83521\cdot 2298912^8)\rfloor &=& 1+\lfloor \log_{10} (83521\cdot 2298912^8)\rfloor\\
		&=&  1+\lfloor \log_{10} (8.3521\cdot10^4 \cdot (2.298912\cdot10^6)^8)\rfloor\\
			&=&  1+\lfloor \log_{10} 8.3521+\log_{10}10^4 + \log_{10}2.298912^8 +\log_{10} (10^6)^8\rfloor\\
			&=& 56
	\end{eqnarray*} 
Now, we compute the last digits of that term, first by plugging in only the last digit of $y$ into expression \eqref{eq:1b}: our calculation gives 21,312,256 as a result. However, printing out the complete term using the command $\text{fprintf}('\%.0f\backslash n',83251*y^8)$, we obtain the following 56-digit number:
\[64,948,367,112,155,225,808,364,994,454,077,172,753,899,719,772,317,155,328.\] 

Comment: The value calculated using direct method should be true, since our computer uses 64-bit CPU that is able to show all the digits of the calculated number. There is no precision loss in this case.

\subsection*{(1c)}
Yes, the 64-bit can show the correct value since the number of digit is less than the maximum number of digit the system can represent. The orders of magnitude of other terms are at most 56. 

	
\section*{Problem 2}
In this exercise, we write an algorithm to determine the relative speeds of addition, multiplication, division, exponentiation, and the logarithmic function of our computer. The computer uses Intel(R) Core(TM) i5-1035G4 CPU @ 1.10GHz, 1.50 GHz with 64-bit operating system and 8.00 GB installed RAM. We proceed by generating two matrices $A$ and $B$ of size $10^4\times10^4$ using the rand() function in Matlab. Then, we use them in element-wise operations of addition ($A+B$), multiplication ($A.*B$), division ($A./B$), exponentiation ($A.^B$), as well as the logarithmic function ($\log(A)$). The statements \texttt{tic} and \texttt{toc} are used around them to measure the computation time. This procedure is iterated 100 times, and the average computation time for each operation is computed. The results are as follows:
	\begin{itemize}
	\item Average computation time for variable initiation is 1.54030 seconds
	\item Average computation time for addition is 0.12073 seconds
	\item Average computation time for multiplication is 0.12083 seconds
	\item Average computation time for division is 0.11823 seconds
	\item Average computation time for exponentiation is 3.70586 seconds
	\item Average computation time for log function is 0.91528 seconds
	\end{itemize}
\section*{Problem 3}
	In order to find our machine $\varepsilon$, we follow Ken Judd's definition\daggerfootnote{Kenneth L. Judd, 1998. "Numerical Methods in Economics," MIT Press Books, The MIT Press, p.30} by writing a while loop to subtract (resp. add) progressively smaller numbers $\epsilon$ from (resp. to) 1 until the condition $1+\epsilon > 1 > 1-\epsilon$ is no longer satisfied. Repeat the exercise using 0.001 and 1000 instead of 1. We verify our results by comparing them with the epsilons delivered by the built-in Matlab function \texttt{eps(x)}, and the exponents with \texttt{log2(eps(x))}. The results are shown on Table \ref{tab:1}.
	
	\begin{table}[h]
		\centering
		\begin{tabular}{ | c | c | c |}
			\hline
			\hline
			$x$ & $\varepsilon(x)$ - decimal & $\varepsilon(x)$ - exponential \\	
			\hline
			0.001 & 2.16840434497101e-19 & $2^{-62}$ \\ \hline
			1 & 2.22044604925031e-16 & $2^{-52}$ \\ \hline
			1000 & 1.13686837721616e-13 & $2^{-43}$ \\
			\hline
			\hline
		\end{tabular} 
		\caption{Machine $\varepsilon$ for diverse values of $x$}
		\label{tab:1}
	\end{table}
Comment: As $x$ increases, the machine $\varepsilon(x)$ also does increase. This is to be expected, since $\varepsilon(x)\approx x\varepsilon(1)$. Thus, $\varepsilon(1000)>\varepsilon(1)>\varepsilon(0.001)$.
	
\section*{Problem 4} 
We wrote a loop to evaluate the convergence of the following sequences: 
\begin{enumerate}
	\item[(4a)] $x_k=\sum_{k=1}^{n}\frac{1}{2^n}$, where $\lim_{k\rightarrow\infty}x_k=1$
	\item[(4b)] $y_k=\sum_{k=1}^{n}\frac{1}{n}$, where $\lim_{k\rightarrow\infty}y_k=\infty$
\end{enumerate}
We use absolute ($||(x_{k+1}-x_k)||$) and relative ($||(x_{k+1}-x_k)||/(1+||x_k||)$) convergence criteria, and tolerance distance $\delta\in\{10^{-2},10^{-4},10^{-6}\}$ for the stopping rule. We limit the maximum number of iterations to 100,000. The number of iterations before convergence, as well the final guess are reported on Table \ref{tab:2} and \ref{tab:3} for $x_k$ and $y_k$, respectively.

	\begin{table}[h]
	\centering
	\begin{tabular}{|c | c | c | c |}
		\hline
		\hline
		$\delta	$							&$10^{-2}$ 			&$10^{-4}$ 			&$10^{-6}$          \\
		\hline
		no. of iteration - absolute criteria&6					&13					&19               \\
		final guess - absolute criteria		&0.9921875	&0.99993896484375	&0.999999046325684\\
		no. of iteration - relative criteria&5					&12					&18               \\
		final guess - relative criteria		&0.984375	&0.9998779296875	&0.999998092651367\\
		\hline
		\hline
	\end{tabular} 
	\caption{Convergence for $x_k$}
	\label{tab:2}
\end{table}

	\begin{table}[h]
	\centering
	\begin{tabular}{|c | c | c | c |}
		\hline
		\hline
		$\delta	$							&$10^{-2}$ 			&$10^{-4}$ 			&$10^{-6}$          \\
		\hline
		no. of iteration - absolute criteria&99					&9,999				&100,000 \\
		final guess - absolute criteria		&5.18737751763962	&9.78760603604435	&12.0901561297633\\
		no. of iteration - relative criteria&21					&1,158				&77,880 \\
		final guess - relative criteria		&3.69081325021728	&7.63295985258894	&11.8401593846097\\
		\hline
		\hline
		\end{tabular} 
	\caption{Convergence for $y_k$}
	\label{tab:3}
\end{table}

Comment: Table \ref{tab:2} shows that both absolute and relative convergence criteria lead to a number of iterations differing by one unit for the sequence $x_k$. Already at the tolerance level of $\delta=10^{-2}$, we see that the final guesses for $x_k$ using both convergence criteria are very close to the true limit 1. As tolerance distance $\delta$ lowers, these numbers get even closer to 1. Table \ref{tab:3} shows that both absolute and relative criteria lead to the significantly different number of iterations for the sequence $y_k$. Since $y_k$ is a divergent sequence, the final guess will be higher (with no upper bound), the more we iterate. By lowering tolerance distance $\delta$, the algorithm needs to iterate longer in order to satisfy the stopping rule, which in turn results in higher final guess. Since there is no upper bound for $y_k$, we can always make the tolerance distance $\delta$ even lower, and obtain even higher final guess for $y_k$. Based on these numbers, we see that $y_k$ goes to infinity very slowly. 

\end{document}

